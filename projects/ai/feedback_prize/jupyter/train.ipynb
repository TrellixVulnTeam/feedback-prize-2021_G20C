{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import glob\n",
    "import spacy\n",
    "import sys\n",
    "sys.path.append('../../../../utils')\n",
    "sys.path.append('..')\n",
    "import gezi\n",
    "from gezi import tqdm\n",
    "from src.eval import calc_f1\n",
    "pd.set_option('display.float_format', lambda x: '%.0f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from absl import flags\n",
    "FLAGS = flags.FLAGS\n",
    "from transformers import AutoTokenizer\n",
    "from src import config\n",
    "from src.model import Model\n",
    "from src.dataset import Dataset\n",
    "from src.util import get_preds\n",
    "import src.eval as ev\n",
    "import melt as mt\n",
    "import husky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| config.py:54 in init_()\n",
      "    FLAGS.backbone: '../input/allenai/longformer-base-4096'\n",
      "ic| config.py:54 in init_()\n",
      "    FLAGS.backbone: '../input/allenai/longformer-base-4096'\n",
      "ic| config.py:63 in init()\n",
      "    FLAGS.valid_files[:2]: ['../input/feedback-prize-2021/tfrecords/train/70.164.tfrec',\n",
      "                            '../input/feedback-prize-2021/tfrecords/train/90.164.tfrec']\n",
      "ic| init.py:200 in init()\n",
      "    tf.__version__: '2.6.2'\n",
      "    torch.__version__: '1.9.0'\n",
      "ic| init.py:334 in init()\n",
      "    FLAGS.model_dir: '../working/offline/0/base.20211227-083507'\n",
      "    FLAGS.log_dir: '../working/offline/0/base.20211227-083507'\n",
      "2021-12-27 08:35:08 0:00:00 batch size is shrink by 4 for each gpu to make total insts per step still 32\n",
      "2021-12-27 08:35:08 0:00:00 fcntl.floc with lock_file /home/huigecheng/.melt.lock (If hang here means other programs calling melt.init have not finished yet)\n",
      "2021-12-27 08:35:08 0:00:00 Tf dataset and Tf model train in Eager mode, keras True, distributed:False\n",
      "2021-12-27 08:35:08 0:00:00 log_level: 20 (try --debug to show more or --log_level=(> 20) to show less(no INFO), try --verbose to show train/valid loss intervaly)\n",
      "ic| init.py:1135 in init()\n",
      "    global_batch_size: 32\n",
      "    acc_steps: 1\n",
      "    replica_batch_size: 8\n",
      "    eval_batch_size: 32\n",
      "    num_gpus: 4\n",
      "    gpus: [3, 2, 1, 0]\n",
      "    CUDA_VISIABLE_DEVICES: [3, 2, 1, 0]\n",
      "    work_mode: 'train'\n",
      "    seed: 1024\n",
      "2021-12-27 08:35:27 0:00:18 model: [base.20211227-083507] model_dir: [../working/offline/0/base.20211227-083507]\n",
      "ic| init.py:1420 in init()\n",
      "    gezi.get('precision_policy_name'): 'float32'\n",
      "2021-12-27 08:35:31 0:00:23 wand_url: https://wandb.ai/chenghuige/feedback_prize/runs/1h5lgl5f\n",
      "All model checkpoint layers were used when initializing TFLongformerModel.\n",
      "\n",
      "All the layers of TFLongformerModel were initialized from the model checkpoint at ../input/allenai/longformer-base-4096.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFLongformerModel for predictions without further training.\n",
      "ic| model.py:30 in __init__()- num_classes: 15\n"
     ]
    }
   ],
   "source": [
    "FLAGS([''])\n",
    "FLAGS.mts = True\n",
    "config.init_()\n",
    "config.init()\n",
    "mt.init()\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFLongformerModel.\n",
      "\n",
      "All the layers of TFLongformerModel were initialized from the model checkpoint at ../input/allenai/longformer-base-4096.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFLongformerModel for predictions without further training.\n",
      "ic| model.py:30 in __init__()- num_classes: 15\n",
      "2021-12-27 08:36:22 0:01:14 Round: 0 mode: None train_input:[None] valid_input:[None] train_dirs:[1] valid_dir: None do_valid: True do_test: True\n",
      "ic| train.py:1082 in train()\n",
      "    len(inputs): 76\n",
      "    inputs[:2]: ['../input/feedback-prize-2021/tfrecords/train/78.164.tfrec',\n",
      "                 '../input/feedback-prize-2021/tfrecords/train/14.164.tfrec']\n",
      "2021-12-27 08:36:32 0:01:24 Model: \"model_2\"\n",
      "2021-12-27 08:36:32 0:01:24 _________________________________________________________________\n",
      "2021-12-27 08:36:32 0:01:24 Layer (type)                 Output Shape              Param #   \n",
      "2021-12-27 08:36:32 0:01:24 =================================================================\n",
      "2021-12-27 08:36:32 0:01:24 tf_longformer_model_2 (TFLon multiple                  148659456 \n",
      "2021-12-27 08:36:32 0:01:24 _________________________________________________________________\n",
      "2021-12-27 08:36:32 0:01:24 multi_dropout_4 (MultiDropou multiple                  58445     \n",
      "2021-12-27 08:36:32 0:01:24 _________________________________________________________________\n",
      "2021-12-27 08:36:32 0:01:24 multi_dropout_5 (MultiDropou multiple                  58605     \n",
      "2021-12-27 08:36:32 0:01:24 =================================================================\n",
      "2021-12-27 08:36:32 0:01:24 Total params: 148,776,506\n",
      "2021-12-27 08:36:32 0:01:24 Trainable params: 148,776,506\n",
      "2021-12-27 08:36:32 0:01:24 Non-trainable params: 0\n",
      "2021-12-27 08:36:32 0:01:24 _________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d753631ec5524bf082d79af669cead90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "get_num_records:   0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| train.py:1192 in train()\n",
      "    len(valid_inputs): 19\n",
      "    valid_inputs[:2]: ['../input/feedback-prize-2021/tfrecords/train/0.165.tfrec',\n",
      "                       '../input/feedback-prize-2021/tfrecords/train/5.165.tfrec']\n",
      "2021-12-27 08:36:33 0:01:25 num_train_examples: 12475 num_steps_per_epoch: 390\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa6c5b0095145c4b1e8910326368880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "get_num_records:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-27 08:36:33 0:01:25 num_valid_examples: 3119 num_valid_steps_per_epoch: 98\n",
      "2021-12-27 08:36:33 0:01:25 round: 0 loss_fn: <function Model.get_loss_fn.<locals>.loss_fn at 0x7f5efb696598>\n",
      "2021-12-27 08:36:33 0:01:25 optimizer: <husky.optimization.AdamWeightDecay object at 0x7f5efb5185f8> lr: <husky.optimization.WarmUp object at 0x7f5efb5180f0> init_lr: 0.0001\n",
      "2021-12-27 08:36:33 0:01:25 total_steps: 1170 warmup_steps: 117 end_lr: 1e-07\n",
      "2021-12-27 08:36:34 0:01:25 before loading, total params: 148776506, l2:0.003339\n",
      "2021-12-27 08:36:34 0:01:25 No model.h5 or other weights file found in ../working/offline/0/base.20211227-083507\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "949e356decd4498b8214b0488604c2fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/3 [00:00<?, ?epochs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d25ecba32b499a831da30d7396275a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[base.20211227-083507] Epochs:3:   0%|          | 0/1170 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fit = mt.fit\n",
    "strategy = mt.distributed.get_strategy()\n",
    "with strategy.scope():    \n",
    "  model = Model()\n",
    "  \n",
    "  fit(model,  \n",
    "      Dataset=Dataset,\n",
    "      eval_fn=ev.evaluate\n",
    "      ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_model().save_weights(f'{FLAGS.model_dir}/model2.h5')\n",
    "    \n",
    "dataset_meta_root = '..'\n",
    "os.system(f'cp {dataset_meta_root}/dataset-metadata.json {FLAGS.model_dir}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e0662240fc4b465c4349360a9dfc6a6e2ea04641a433e7a5d8ebbabdd2e0b514"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
