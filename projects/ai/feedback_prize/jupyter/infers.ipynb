{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import glob\n",
    "import spacy\n",
    "import sys\n",
    "import gc\n",
    "from IPython.display import display\n",
    "sys.path.append('../../../../utils')\n",
    "sys.path.append('..')\n",
    "import gezi\n",
    "from gezi import tqdm\n",
    "from src.util import *\n",
    "pd.set_option('display.float_format', lambda x: '%.0f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../../../../utils')\n",
    "sys.path.append('../../../../third')\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from absl import flags\n",
    "FLAGS = flags.FLAGS\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from src import config\n",
    "from src.util import *\n",
    "import melt as mt\n",
    "import numpy as np\n",
    "from gezi import tqdm\n",
    "import gezi\n",
    "import husky\n",
    "import lele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = '40'\n",
    "mns = [\n",
    "  'mid.electra.sm=start.mui.mis=mid,end.bs=16.y',\n",
    "  # 'mid.roberta.sm=start.mui.mis=mid,end.bs=16.y',\n",
    "  # 'mid.xlnet.sm=start.mui.mis=mid,end.bs=16.y',\n",
    "  # 'mid.deberta-v3.sm=start.mui.mis=end.bs=16.y',\n",
    "]\n",
    "root = f'../working/online/{v}/0/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dirs = [f'{root}/{mn}' for mn in mns]\n",
    "model_dir = model_dirs[0]\n",
    "gezi.init_flags()\n",
    "batch_sizes = [32] * 100\n",
    "weights = [1] * 100\n",
    "num_tf_models = len([x for x in model_dirs if 'tf.' in x])\n",
    "num_tf_models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = {}\n",
    "def get_inputs(backbone, mode='test', reset=True, sort=True, double_times=0, split_method=None, multi_inputs=None):\n",
    "  split_method = split_method if split_method is not None else FLAGS.split_method\n",
    "  multi_inputs = multi_inputs if multi_inputs is not None else FLAGS.multi_inputs\n",
    "  if multi_inputs:\n",
    "    split_methods = [split_method] + FLAGS.multi_inputs_srcs\n",
    "    res = {}\n",
    "    for i, split_method in enumerate(split_methods):\n",
    "      res[i] = get_inputs(backbone, mode, reset, sort, double_times, split_method, multi_inputs=False)\n",
    "    res.update({\n",
    "      'id': res[0]['id'],\n",
    "      'num_words': res[0]['num_words']\n",
    "    })\n",
    "    return res\n",
    "  # TODO 内部之间支持可以减少encode分词调用次数\n",
    "  multi_inputs = False\n",
    "  #--- for online infer\n",
    "  FLAGS.remove_br = False\n",
    "  if backbone in test_inputs and not reset:\n",
    "    return test_inputs[backbone]\n",
    "  else:\n",
    "    tokenizer = get_tokenizer(backbone)\n",
    "    files = os.listdir(f'../input/feedback-prize-2021/{mode}')\n",
    "    TEST_IDS = [f.replace('.txt','') for f in files if 'txt' in f]\n",
    "    ic(len(TEST_IDS), torch.cuda.is_available(), tokenizer.padding_side)\n",
    "    if len(TEST_IDS) < 10 and torch.cuda.is_available() and double_times:\n",
    "      for _ in range(double_times):\n",
    "        TEST_IDS += TEST_IDS\n",
    "    ic(len(TEST_IDS))\n",
    "    MAX_LEN = FLAGS.max_len\n",
    "    input_ids = np.zeros((len(TEST_IDS), MAX_LEN), dtype='int32')\n",
    "    attention_mask = np.zeros((len(TEST_IDS), MAX_LEN), dtype='int32')\n",
    "    word_ids_list = []\n",
    "    re_pos_list = []\n",
    "    num_words_list = []\n",
    "   \n",
    "    for id_num in tqdm(range(len(TEST_IDS))):\n",
    "      n = TEST_IDS[id_num]\n",
    "      name = f'../input/feedback-prize-2021/{mode}/{n}.txt'\n",
    "      txt = open(name, 'r').read()\n",
    "      res = encode(txt, tokenizer, split_method=split_method, multi_inputs=multi_inputs) \n",
    "      input_ids[id_num,] = res['input_ids']\n",
    "      attention_mask[id_num,] = res['attention_mask']\n",
    "     \n",
    "      word_ids_list.append(res['word_ids'])\n",
    "      re_pos_list.append(res['relative_positions'])\n",
    "      num_words_list.append(len(txt.split()))\n",
    "    \n",
    "    inputs = {\n",
    "      'id': TEST_IDS,\n",
    "      'input_ids': input_ids,\n",
    "      'attention_mask': attention_mask,\n",
    "      'word_ids': word_ids_list,\n",
    "      'relative_positions': re_pos_list,\n",
    "      'num_words': num_words_list,\n",
    "    }    \n",
    "    keys = list(inputs.keys())\n",
    "    if sort:\n",
    "      df = pd.DataFrame({k: list(inputs[k]) for k in keys})\n",
    "      df = df.sort_values('num_words')\n",
    "      inputs = {\n",
    "        k: np.asarray(df[k].values) for k in keys\n",
    "      }\n",
    "    if not reset:\n",
    "      test_inputs[backbone] = inputs\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ensembler = Ensembler()\n",
    "for i, model_dir in tqdm(enumerate(model_dirs), total=len(model_dirs)):\n",
    "  ic(model_dir)\n",
    "  gezi.restore_configs(model_dir)\n",
    "  FLAGS.wandb = False\n",
    "  ic(FLAGS.torch)\n",
    "  ic(FLAGS.model_dir)\n",
    "  FLAGS.backbone = '../input/' + FLAGS.backbone.split('/')[-1]\n",
    "  ic(FLAGS.backbone, FLAGS.max_len)\n",
    "  model = get_model()\n",
    "  gezi.load_weights(model, model_dir)\n",
    "  ic(gezi.get_mem_gb())\n",
    "  display(pd.read_csv(f'{model_dir}/metrics.csv'))\n",
    "  inputs = get_inputs(FLAGS.backbone)\n",
    " \n",
    "  # not for roberta-large tf can use 32 bs, torch can use even larger bs, for longformer tf only 8, torch can use 32\n",
    "  #   batch_size = 32 if FLAGS.torch else 8\n",
    "  batch_size = 32 \n",
    "  p = predict(model, inputs, batch_sizes[i], dynamic_keys=['input_ids'], mask_key='attention_mask')\n",
    "  p.update({\n",
    "    'id': inputs['id'],\n",
    "    'word_ids': inputs['word_ids'],\n",
    "    'num_words': inputs['num_words']\n",
    "  })\n",
    "  convert_res(p)\n",
    "  ensembler.add(p, weights[i])\n",
    "  if len(inputs['id']) < 1000:\n",
    "    df = get_preds(p)\n",
    "    display(df)\n",
    "    df = link_evidence(df)\n",
    "    display(df)\n",
    "    \n",
    "  del model\n",
    "  del inputs\n",
    "  if FLAGS.torch:\n",
    "    torch.cuda.empty_cache()\n",
    "  else:\n",
    "    # only the last tf model should cuda close\n",
    "    if i + 1 == num_tf_models:\n",
    "      cuda.select_device(0)\n",
    "      cuda.close()\n",
    "  gc.collect()\n",
    "\n",
    "p = ensembler.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_preds(p)\n",
    "display(df)\n",
    "df = link_evidence(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'{model_dir}/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0ea47313c94383bf8f680c96aa3342ec5666a2ae764c7d77e71df945692dda57"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
