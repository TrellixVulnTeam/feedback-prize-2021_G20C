{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-01-02T23:30:34.800708Z",
     "iopub.status.busy": "2022-01-02T23:30:34.800242Z",
     "iopub.status.idle": "2022-01-02T23:30:37.518711Z",
     "shell.execute_reply": "2022-01-02T23:30:37.517458Z",
     "shell.execute_reply.started": "2022-01-02T23:30:34.800650Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os  \n",
    "import traceback\n",
    "\n",
    "RECORDS_PATH = '../input/tfrecords'\n",
    "\n",
    "!ln -s ../input/feedback ./src\n",
    "\n",
    "if os.path.exists('/kaggle'):\n",
    "  sys.path.append('/kaggle/input/pikachu/utils')\n",
    "  sys.path.append('/kaggle/input/pikachu/third')\n",
    "  sys.path.append('.')\n",
    "  from kaggle_datasets import KaggleDatasets\n",
    "#   try:\n",
    "#     RECORDS_PATH = KaggleDatasets().get_gcs_path('toxic-multi-tfrecords') + '/tfrecords/tfrecords'\n",
    "#     os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "#   except Exception:\n",
    "#     print(traceback.format_exc())\n",
    "#     pass\n",
    "!ls ../input\n",
    "RECORDS_PATH\n",
    "!ls ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-02T23:30:37.522329Z",
     "iopub.status.busy": "2022-01-02T23:30:37.521482Z",
     "iopub.status.idle": "2022-01-02T23:30:47.869543Z",
     "shell.execute_reply": "2022-01-02T23:30:47.868366Z",
     "shell.execute_reply.started": "2022-01-02T23:30:37.522278Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q icecream --no-index --find-links=file:///kaggle/input/icecream/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-02T23:30:47.872753Z",
     "iopub.status.busy": "2022-01-02T23:30:47.872367Z",
     "iopub.status.idle": "2022-01-02T23:30:47.882916Z",
     "shell.execute_reply": "2022-01-02T23:30:47.881792Z",
     "shell.execute_reply.started": "2022-01-02T23:30:47.872694Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "from absl import flags\n",
    "FLAGS = flags.FLAGS\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from src import config\n",
    "from src.torch_model import Model as TorchModel\n",
    "from src.model import Model\n",
    "from src.util import get_preds\n",
    "import melt as mt\n",
    "import numpy as np\n",
    "from gezi import tqdm\n",
    "import gezi\n",
    "import husky\n",
    "import lele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-02T23:30:47.886964Z",
     "iopub.status.busy": "2022-01-02T23:30:47.886339Z",
     "iopub.status.idle": "2022-01-02T23:31:53.677183Z",
     "shell.execute_reply": "2022-01-02T23:31:53.673763Z",
     "shell.execute_reply.started": "2022-01-02T23:30:47.886916Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dir = '../input/feedback-model'\n",
    "argv = open(f'{model_dir}/command.txt').readline().strip().split()\n",
    "FLAGS(argv)\n",
    "FLAGS.wandb = False\n",
    "config.init_()\n",
    "FLAGS.backbone = '../input/' + FLAGS.backbone.split('/')[-1]\n",
    "ic(FLAGS.backbone)\n",
    "# mt.init()\n",
    "MAX_LEN = FLAGS.max_len\n",
    "tokenizer = AutoTokenizer.from_pretrained(FLAGS.backbone, add_prefix_space=True)\n",
    "if not FLAGS.torch:\n",
    "  model = Model().build_model()\n",
    "  model.load_weights(f'{model_dir}/model2.h5')\n",
    "else:\n",
    "  model = TorchModel()\n",
    "  lele.load(model, f'{model_dir}/model.pt')\n",
    "open(f'{model_dir}/path.txt').readline().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-01-02T23:31:53.678577Z",
     "iopub.status.idle": "2022-01-02T23:31:53.680019Z",
     "shell.execute_reply": "2022-01-02T23:31:53.679633Z",
     "shell.execute_reply.started": "2022-01-02T23:31:53.679598Z"
    }
   },
   "outputs": [],
   "source": [
    "files = os.listdir('../input/feedback-prize-2021/test')\n",
    "TEST_IDS = [f.replace('.txt','') for f in files if 'txt' in f]\n",
    "if len(TEST_IDS) < 10:\n",
    "  for _ in range(6):\n",
    "    TEST_IDS += TEST_IDS\n",
    "print('There are',len(TEST_IDS),'test texts.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-02T23:32:47.468584Z",
     "iopub.status.busy": "2022-01-02T23:32:47.468193Z",
     "iopub.status.idle": "2022-01-02T23:32:49.941448Z",
     "shell.execute_reply": "2022-01-02T23:32:49.940425Z",
     "shell.execute_reply.started": "2022-01-02T23:32:47.468536Z"
    }
   },
   "outputs": [],
   "source": [
    "# CONVERT TEST TEXT TO TOKENS\n",
    "input_ids = np.zeros((len(TEST_IDS), MAX_LEN), dtype='int32')\n",
    "attention_mask = np.zeros((len(TEST_IDS), MAX_LEN), dtype='int32')\n",
    "word_ids_list = []\n",
    "for id_num in tqdm(range(len(TEST_IDS))):\n",
    "  n = TEST_IDS[id_num]\n",
    "  name = f'../input/feedback-prize-2021/test/{n}.txt'\n",
    "  txt = open(name, 'r').read()\n",
    "  encoding = tokenizer(txt.split(),\n",
    "                        is_split_into_words=True,\n",
    "                        padding='max_length', \n",
    "                        truncation=True, \n",
    "                        max_length=MAX_LEN)\n",
    "  input_ids[id_num,] = encoding['input_ids']\n",
    "  attention_mask[id_num,] = encoding['attention_mask']\n",
    "  word_ids = encoding.word_ids()\n",
    "  word_ids = [-1 if x is None else x for x in word_ids]\n",
    "  word_ids_list.append(word_ids)\n",
    "inputs = {\n",
    "   'input_ids': input_ids,\n",
    "   'attention_mask': attention_mask\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-02T23:32:51.945480Z",
     "iopub.status.busy": "2022-01-02T23:32:51.944501Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO for longformer large seems torch can use much larger bs for inference? for torch large+32 use 13.9G \n",
    "BATCH_SIZE = 8 if not FLAGS.torch else 32 \n",
    "# INFER TEST TEXTS\n",
    "if not FLAGS.torch:\n",
    "  inputs = {\n",
    "   'tokens': input_ids,\n",
    "   'attention': attention_mask\n",
    "   }\n",
    "  dataset = tf.data.Dataset.from_tensor_slices(inputs).batch(BATCH_SIZE)\n",
    "  strategy = mt.distributed.get_strategy()\n",
    "  with strategy.scope(): \n",
    "    p = model.predict(dataset, callbacks=[husky.TQDMProgressBar()])\n",
    "else:\n",
    "  dataset = Dataset.from_dict(inputs)\n",
    "  dataset.set_format(type='torch', device='cuda')\n",
    "  dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "  p = lele.predict(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-01-02T23:31:53.686630Z",
     "iopub.status.idle": "2022-01-02T23:31:53.687876Z",
     "shell.execute_reply": "2022-01-02T23:31:53.687516Z",
     "shell.execute_reply.started": "2022-01-02T23:31:53.687483Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = np.argmax(p['pred'],axis=-1)\n",
    "scores = gezi.softmax(p['pred'], axis=-1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-01-02T23:31:53.689517Z",
     "iopub.status.idle": "2022-01-02T23:31:53.690712Z",
     "shell.execute_reply": "2022-01-02T23:31:53.690402Z",
     "shell.execute_reply.started": "2022-01-02T23:31:53.690368Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'start_logits' in p:\n",
    "  preds = (preds, gezi.softmax(p['start_logits'], axis=-1))\n",
    "df = get_preds(TEST_IDS, preds, word_ids_list, scores)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-01-02T23:31:53.692166Z",
     "iopub.status.idle": "2022-01-02T23:31:53.693506Z",
     "shell.execute_reply": "2022-01-02T23:31:53.693194Z",
     "shell.execute_reply.started": "2022-01-02T23:31:53.693155Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
